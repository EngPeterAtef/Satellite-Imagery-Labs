{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Example of building a simple CNN for CIFAR10 dataset. "
      ],
      "metadata": {
        "id": "mEXpCSS3f1Dq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlwwnNokdEsG",
        "outputId": "98df9c33-04f7-4a72-ac80-21c8252b786a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 90321476.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[1,  2000] loss: 2.274\n",
            "[1,  4000] loss: 1.923\n",
            "[1,  6000] loss: 1.709\n",
            "[1,  8000] loss: 1.586\n",
            "[1, 10000] loss: 1.536\n",
            "[1, 12000] loss: 1.473\n",
            "[2,  2000] loss: 1.399\n",
            "[2,  4000] loss: 1.398\n",
            "[2,  6000] loss: 1.325\n",
            "[2,  8000] loss: 1.327\n",
            "[2, 10000] loss: 1.298\n",
            "[2, 12000] loss: 1.282\n",
            "[3,  2000] loss: 1.217\n",
            "[3,  4000] loss: 1.193\n",
            "[3,  6000] loss: 1.206\n",
            "[3,  8000] loss: 1.198\n",
            "[3, 10000] loss: 1.196\n",
            "[3, 12000] loss: 1.178\n",
            "[4,  2000] loss: 1.125\n",
            "[4,  4000] loss: 1.101\n",
            "[4,  6000] loss: 1.103\n",
            "[4,  8000] loss: 1.118\n",
            "[4, 10000] loss: 1.097\n",
            "[4, 12000] loss: 1.105\n",
            "[5,  2000] loss: 1.038\n",
            "[5,  4000] loss: 1.035\n",
            "[5,  6000] loss: 1.025\n",
            "[5,  8000] loss: 1.044\n",
            "[5, 10000] loss: 1.040\n",
            "[5, 12000] loss: 1.054\n",
            "[6,  2000] loss: 0.962\n",
            "[6,  4000] loss: 0.963\n",
            "[6,  6000] loss: 0.978\n",
            "[6,  8000] loss: 0.983\n",
            "[6, 10000] loss: 0.997\n",
            "[6, 12000] loss: 0.982\n",
            "[7,  2000] loss: 0.918\n",
            "[7,  4000] loss: 0.888\n",
            "[7,  6000] loss: 0.923\n",
            "[7,  8000] loss: 0.962\n",
            "[7, 10000] loss: 0.936\n",
            "[7, 12000] loss: 0.958\n",
            "[8,  2000] loss: 0.858\n",
            "[8,  4000] loss: 0.898\n",
            "[8,  6000] loss: 0.920\n",
            "[8,  8000] loss: 0.900\n",
            "[8, 10000] loss: 0.909\n",
            "[8, 12000] loss: 0.903\n",
            "[9,  2000] loss: 0.819\n",
            "[9,  4000] loss: 0.829\n",
            "[9,  6000] loss: 0.862\n",
            "[9,  8000] loss: 0.857\n",
            "[9, 10000] loss: 0.897\n",
            "[9, 12000] loss: 0.893\n",
            "[10,  2000] loss: 0.773\n",
            "[10,  4000] loss: 0.823\n",
            "[10,  6000] loss: 0.848\n",
            "[10,  8000] loss: 0.848\n",
            "[10, 10000] loss: 0.846\n",
            "[10, 12000] loss: 0.855\n",
            "[11,  2000] loss: 0.759\n",
            "[11,  4000] loss: 0.776\n",
            "[11,  6000] loss: 0.829\n",
            "[11,  8000] loss: 0.813\n",
            "[11, 10000] loss: 0.816\n",
            "[11, 12000] loss: 0.840\n",
            "[12,  2000] loss: 0.721\n",
            "[12,  4000] loss: 0.748\n",
            "[12,  6000] loss: 0.777\n",
            "[12,  8000] loss: 0.803\n",
            "[12, 10000] loss: 0.785\n",
            "[12, 12000] loss: 0.813\n",
            "[13,  2000] loss: 0.705\n",
            "[13,  4000] loss: 0.724\n",
            "[13,  6000] loss: 0.749\n",
            "[13,  8000] loss: 0.774\n",
            "[13, 10000] loss: 0.772\n",
            "[13, 12000] loss: 0.814\n",
            "[14,  2000] loss: 0.690\n",
            "[14,  4000] loss: 0.728\n",
            "[14,  6000] loss: 0.726\n",
            "[14,  8000] loss: 0.750\n",
            "[14, 10000] loss: 0.750\n",
            "[14, 12000] loss: 0.777\n",
            "[15,  2000] loss: 0.661\n",
            "[15,  4000] loss: 0.704\n",
            "[15,  6000] loss: 0.725\n",
            "[15,  8000] loss: 0.740\n",
            "[15, 10000] loss: 0.730\n",
            "[15, 12000] loss: 0.779\n",
            "[16,  2000] loss: 0.627\n",
            "[16,  4000] loss: 0.694\n",
            "[16,  6000] loss: 0.703\n",
            "[16,  8000] loss: 0.737\n",
            "[16, 10000] loss: 0.733\n",
            "[16, 12000] loss: 0.750\n",
            "[17,  2000] loss: 0.619\n",
            "[17,  4000] loss: 0.671\n",
            "[17,  6000] loss: 0.696\n",
            "[17,  8000] loss: 0.709\n",
            "[17, 10000] loss: 0.739\n",
            "[17, 12000] loss: 0.741\n",
            "[18,  2000] loss: 0.617\n",
            "[18,  4000] loss: 0.638\n",
            "[18,  6000] loss: 0.700\n",
            "[18,  8000] loss: 0.681\n",
            "[18, 10000] loss: 0.735\n",
            "[18, 12000] loss: 0.741\n",
            "[19,  2000] loss: 0.612\n",
            "[19,  4000] loss: 0.645\n",
            "[19,  6000] loss: 0.666\n",
            "[19,  8000] loss: 0.717\n",
            "[19, 10000] loss: 0.710\n",
            "[19, 12000] loss: 0.714\n",
            "[20,  2000] loss: 0.601\n",
            "[20,  4000] loss: 0.639\n",
            "[20,  6000] loss: 0.641\n",
            "[20,  8000] loss: 0.683\n",
            "[20, 10000] loss: 0.690\n",
            "[20, 12000] loss: 0.729\n",
            "Finished Training\n",
            "Accuracy of the network on the 10000 test images: 61 %\n"
          ]
        }
      ],
      "source": [
        "''' \n",
        "We import the necessary libraries and modules for our code.\n",
        "We use PyTorch as our deep learning framework, \n",
        "nn module for defining neural networks, \n",
        "optim module for optimization algorithms, \n",
        "and transforms module for data preprocessing.\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the CNN architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        CNN Architecture: 2 convolutional layers, 1 max pool, 2 fully connected layer, \n",
        "        and final fully connected layer to classify 10 different classes\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "        \"\"\"\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "\"\"\"\n",
        "Creates a data transformation pipeline using the transforms.Compose function from the PyTorch library.\n",
        "\n",
        "\"\"\"\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize((32, 32)),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Train the network\n",
        "for epoch in range(20):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    \n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Test the network on the test data\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))\n",
        "\n",
        "# Save the trained model\n",
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirement\n",
        "Use EuroSAT-RGB dataset of satellite images.\n",
        "\n",
        "Preprocess the dataset to prepare it for training, including resizing the images, normalizing the pixel values, and splitting the dataset into training and validation sets.\n",
        "\n",
        "Build a simple CNN using pytorch. The CNN should include one or more convolutional layers, pooling layers, and one or more dense layers for classification.\n",
        "\n",
        "Train the CNN on the training set using an appropriate loss function and optimizer. Evaluate the performance of the CNN on the validation set, using appropriate evaluation metrics such as accuracy.\n",
        "\n",
        "Fine-tune the CNN by adjusting the hyperparameters such as the learning rate, number of layers, and number of filters to optimize its performance.\n",
        "\n",
        "Finally, test the CNN on a new set of unlabeled images to evaluate its real-world performance.\n",
        "\n",
        "# Grading will be on overall steps and the final accuracy."
      ],
      "metadata": {
        "id": "DAaE_BggmDFG"
      }
    }
  ]
}